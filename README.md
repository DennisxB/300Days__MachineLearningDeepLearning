# My Journey of 300DaysofData in Machine Learning and Deep Learning üöÄ

**üéØ Day 1 of #300DaysofData!:**

**Ordinary Linear Regression**:<br>
In my journey of Machine Learning and Deep Learning, today I learned and implemented Ordinary Linear regression, Parameter Estimation, Minimizing Loss and Maximizing Likelihoods along with the construction and implementation of simple Linear Regression from the book **Machine Learning From Scratch**  and also an explanation of Performance Evaluation for the Estimation of Deterministic Parameters and Maximum Likelihood  Estimation for Multiple Regression from Timothy Schulz. I hope you will find some time reading the topics and books mentioned.
- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Video:
  - [**Detection and Estimation Theory by Timothy Schulz**](https://www.youtube.com/playlist?list=PLXo-ki9WzLdjOFbvGz_TXCk7syaJ_2seq)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day1a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day1b.png)
<hr>

**üéØ Day 2 of #300DaysofData!:**
 
**Linear Regression Extensions:**<br> 
Today, I learned how to construct and implement the Extensions of Linear Regression which includes Ridge Regression, LASSO Regression, Bayesian Regression and Generalized Linear Models(GLMs) from the book **Machine Learning From Scratch**. I revised Matrix Calculus; dealing with multiple parameters, multiple observations, multiple loss functions and also Bayesian Estimators and Performance Estimation for Random Variables. Below is a snapshot of Regularised Regression, Bayesian Regression and Possion Regression(GLMs) using Python and Numpy Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Video:
  - [**Detection and Estimation Theory by Timothy Schulz**](https://www.youtube.com/playlist?list=PLXo-ki9WzLdjOFbvGz_TXCk7syaJ_2seq)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2b.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2c.png)
<hr>

**üéØ Day 3 of #300DaysofData!:**

**Discriminative Classifiers**<br>
Today, I learned how to construct and implement Binary and Multiple Logistic Regression, The Perceptron Algorithm, and Fisher‚Äôs Linear Discriminant from the book **Machine Learning From Scratch**. I also read a blog post on the Math and Gradient Descent implementation of Multiclass Logistic Regression in Python by Sofia Yang. Below is a snapshot of Binary and Multiple Logistic Regression, The Perceptron Algorithm, and Fisher‚Äôs Linear Discriminant using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Blog:
  - [**Multiclass Logistic Regression from Scratch**](https://towardsdatascience.com/multiclass-logistic-regression-from-scratch-9cc0007da372)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day3.png)
<hr>

**üéØ Day 4 of #300DaysofData!:**

**Generative Classifiers**<br>
Today, I read about Generative Classifiers which includes Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes and how to construct and implement them from the book **Machine Learning From Scratch**. I also read blog posts to further grasp the underlying Concepts and their Implications of Modelling Assumptions. Below is a snapshot of Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Blog:
  - [**Differences between LDA, QDA and Gaussian Naive Bayes classifiers**](https://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day4.png)
<hr>

**üéØ Day 5 of #300DaysofData!:**

**Decision Trees**:<br>
Today, I read about Decision Tree Regressor and Classifier, how it makes Predictions and its Interpretability, how it uses The CART Algorithm to split instances based on Low Mean Squared Error for Regression tasks, Gini Impurity and Entropy and also Regularization and Hyperparameter tuning, Pruning from the book **Machine Learning From Scratch** and **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Below is a snapshot of Decision Tree Regressor and Classifier implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day5.png)
<hr>

**üéØ Day 6 of #300DaysofData!:**

**Tree Ensemble Methods**:<br>
Ensemble Methods combine the outputs of multiple simple Models which is often called Learners in order to create the fine Model with low variance. Due to their high variance, a decision trees often fail to reach a level of precision comparable to other predictive algorithms and Ensemble Methods also known as Ensemble Learning Algorithms minimize the variance. Today I read and implemented Tree Ensemble Methods such as Bagging  Random Forests and Boosting experimenting with Base Estimators like Naive Bayes, Logistic Regression and Decision Tree, The Discrete AdaBoost Algorithm and AdaBoost for Regression along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. To further broaden my understanding, I used the book **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** to read on Voting Classifiers, Stacking, Out-of-Bag, Random Patches and Subspaces, Feature Importance, Regularization and Hyperparameter tuning of Ensemble Methods to which I applied to a project I have been working on, Exploring Nasa's Turbofan Dataset for Aircraft Predictive Maintenance. Below is a snapshot of Bagging, Random Forest and AdaBoost Classifier implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day6.png)

- Project Notebook: 
[**Turbofan Predictive Maintenance**](#)
<hr>


**üéØ Day 7 of #300DaysofData!:**

**Neural Networks**:<br>
A neural network in this case, artificial neurons called Artificial Neural Network (ANN) is an interconnected group of artificial neurons that uses a mathematical or computational model for information processing. In my journey of Machine Learning and Deep Learning, today I read and implemented Neural Networks along with the Construction of a Feed-Forward Neural Network with the Loop and Matrix Approach which constituted the Model Structure, Communication between Layers, Activation Functions, Optimization of Neural Nets using Back Propagation, Calculating Gradients and Combining Results with the Chain Rule from the same book **Machine Learning From Scratch** . Below is a snapshot of a Feed-Forward Neural Network implemented using Keras. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day7.png)
<hr>

**üéØ Day 8 of #300DaysofData!:**

**Gradient Descent and Cross Validation**:<br>
Gradient descent is an iterative approach to approximating the Parameters that Minimize a Differentiable Loss Function. Cross Validation is a Resampling Procedure used to evaluate Machine Learning Models on a limited Data sample which has a Parameter that splits the data into number of groups. Today, I read and implemented Gradient Descent and Cross validation along with the Construction from the book **Machine Learning From Scratch**. Below is a snapshot of Gradient Descent and Cross Validation using Python and the Numpy Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day8a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day8b.png)
<hr>

**üéØ Day 9 of #300DaysofData!:**

**Problem Framing and Fundermentals of Machine Learning**:<br>
Problem framing is the process of analyzing a problem to isolate the individual elements that need to be addressed to solve it and it also helps determine your project's technical feasibility and provides a clear set of goals and success criteria. Today, I read about Problem Framing, Understanding and Framing ML Problems and also the steps to take when tackling ML problems from Google's Machine learning Crash Course.Types of Machine Learning Systems: Supervised and Unsupervised Learning, Batch and Online Learning, Instance-Based Versus Model-Based Learning and also Main Challenges of ML from the book **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- [**Machine Learning Crash Course**](https://developers.google
<hr>

**üéØ Day 10 of #300DaysofData!:**

Today, I read about Classifications which entailed Training a Binary Classifier, Measuring Accuracy Using Cross-Validation, Multiclass, Multilabel and Multioutput Classification from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Since I have been learning from the same book for sometime now. I revised on Support Vector Machines, Desicion Trees, Ensemble Learning and Random Forests with their in-depth implementations. Below is repository containing the various notebooks including projects.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git))
<hr>

**üéØ Day 11 of #300DaysofData!:**

Today, I read about The Curse of Dimensionality and how it has Effects on Machine Learning Models and the Techniques involved in tackling such problems. Projection and Manifold Learning Approach, PCA, Random Projection and LLE from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Since I have been learning from the same book for sometime now, I revised on Unsupervised Machine Learning: Clustering, Using KMeans, Anomaly Detection Using Gaussian Mixtures, Bayesian Gaussian Mixture Models, Anomaly and Novelty Detection with their in-depth implementations. Below is repository containing the related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)
<hr>

**üéØ Day 12 of #300DaysofData!:**

In my journey of Machine Learning and Deep Learning,
today, I read and implemented Artificial Neural Networks with Keras. Getting Started with The History Behind ANNs, Logical Computations with Neurons, Single Layered Perceptron, Multilayered Perceptron, Auto Diff, the COncept Behind Backpropagation, Regression and Classification MLPs. I also Built an Image Classifier Using the Sequential API, Complex Models Using the Functional API, Using the Subclassing API to Build Dynamic Models, Saving and Restoring a Model, Using Callbacks and TensorBoards. Fine-Tuning a Neural Network with Keras-Tuner from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Below is repository containing the related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)

<hr>

**üéØ Day 13 of #300DaysofData!:**

In my journey of Machine Learning and Deep Learning, today, I read and implemented Training Deep Neural Networks,
A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough
training instances or if th There Various Techniques in tackling these problems, i.e The Vanishing/Exploding Gradients
Problems which includes Glorot and He Initialization, Better Activation Functions such as Leaky RELU, Randomized Leaky ReLU (RReLU), SELU and ELUs, GELU, Swish and Mish, Batch Normalization and Gradient Clipping from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. I also revised on utilising Pretrained Models and Transfer Learning and how they make certain tasks easier, Faster Optimizers and How to avoid Overfitting when training. Below is repository containing the related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)
<hr>

**üéØ Day 14 of #300DaysofData!:**

**Deep Computer Vision Using Convolutional**:<br>
CNNs have managed to achieve superhuman performance on some complex visual tasks. They power Image Search services, Self-Driving Cars, Automatic Video Classification Systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful in performing many other tasks, such as Voice Recognition or Natural Language Processing (NLP). In my journey of Machine Learning and Deep Learning, today, I read and implemented Convolutional Neuron Networks using Tensorflow's Keras from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**, the Building Blocks of CNNs: Convolutional Layers, Filters, Stacking Multiple Feature Maps, Pooling. Below is a snapshot of the Tensorflow implementation of Convolutional layer and Pooling Layer and also its repository containing the related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day14.png)
<hr>

**üéØ Day 15 of #300DaysofData!:**

**CNN Architecture**:<br>
Typical CNN architectures stack a few Convolutional layers (each one generally fol‚Äê
lowed by a ReLU layer), then a Pooling layer, then another few Convolutional layers (+ReLU), then another Pooling Layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more Feature Maps) thanks to the Convolutional layers. today I learned and implemented CNN Architures, Techniques and Construction. I also read about ImageNet Challenge which introduced me to AlexNet, LeNet-5, GoogLeNet, VGGNet, ResNet, Xception, and SENet Architectures from the book **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. I also Constructed ResNet From Scratch using Tensorflow. Below is a snapshot of a CNN Architecture and ResNet From Scratch using Tensorflow and Keras and repository to related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day15.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day16.png)
<hr>

**üéØ Day 16 of #300DaysofData!:**

**Object Detection and Semantic Segmentation**:<br>
In my journey of Machine Learning and Deep Learning, today, I read about  Object Detection and Semantic Segmentation. Object Detection is simply the task of classifying and localizing multiple objects in an image. In Semantic Segmentation, each pixel is classified according to the class of the object it belongs to. I read about Fully Convolutional Networks (FCNs), You Only Look Once (YOLO), Non-Max Suppression, Classification and Localization and the Mean Average Precision (mAP) from the book **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Below is a snapshot of Xception Architecture used for Classification and Localization in Tensorflow and repository to related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day16.png)
<hr>

**üéØ Day 17 of #300DaysofData!:**
**Tensor: Multidimensional Arrays**:<br>
A Tensor is an array that is a Data Structure which stores a collection of numbers that are accessible individually using a index and that can be indexed with multiple indices. On my Journey of Machine Learning and Deep Learning, today, I have read and implemented from the book, **Deep Learning with Pytorch**. I have learned about A Pretrained Neural Network that describes the scenes, NeuralTalk2 Model, Recurrent Neural Network, Torch Hub, Fundamental Building Block: Tensors, The world as Floating Point Numbers, Multidimensional Arrays and Tensors, Lists and Indexing Tensors, Named Tensors, Einsum, Broadcasting and few more Topics related to the same from here. represent that data as tensors. I also went through Fundementals of Pytorch from **Made With ML** for further explanation and implementation to which I was introduced on The Dangers of Transposing (Unintended Consequences), Gradients and Setting up a device to run Tensors. Below is a  simple implementation of the Tensor: Multidimensional Arrays and I hope you find some time to go through the materials mentioned.

- Book:
  - [**Deep Learning with Pytorch**](https://www.manning.com/books/deep-learning-with-pytorch)
- Made With ML :
  - [**Fundamentals of Pytorch**](https://madewithml.com/courses/foundations/pytorch/)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day17a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day17b.png)
<hr>

**üéØ Day 17 of #300DaysofData!:**

**Real-world Data Representation Using Tensors**:

On my Journey to Machine Learnin and Deep Learning, Today I have learned about Real-world Data Representation with Pytorch which entails Working with Images, Adding Color Channels, Changing Layouts, Normalizing Data. I have also learned how to load and Preprocess Volumentric Data, Representing Tabular Data, Preprocessing and Performing Feature Engineering like Ordinal Encoding and One-Hot Encoding on Categorical Variables with Pytorch from the Book **Deep Learning with Pytorch**.Below is a snapshot of the Implementation of  Working with Images, Changing Layouts, Normalizing Data and also Feature Engineering on the Wine Dataset. I hope you find some time to go through the materials mentioned.

- Book:
  - [**Deep Learning with Pytorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day18a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day18b.png)
<hr>

