# My Journey of 300DaysofData in Machine Learning and Deep Learning ðŸš€

**ðŸŽ¯ Day 1 of #300DaysofData!:**

**Ordinary Linear Regression**:<br>
In my journey of Machine Learning and Deep Learning, today I learned and implemented Ordinary Linear regression, Parameter Estimation, Minimizing Loss and Maximizing Likelihoods along with the construction and implementation of simple Linear Regression from the book **Machine Learning From Scratch**  and also an explanation of Performance Evaluation for the Estimation of Deterministic Parameters and Maximum Likelihood  Estimation for Multiple Regression from Timothy Schulz. I hope you will find some time reading the topics and books mentioned.
- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Video:
  - [**Detection and Estimation Theory by Timothy Schulz**](https://www.youtube.com/playlist?list=PLXo-ki9WzLdjOFbvGz_TXCk7syaJ_2seq)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day1a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day1b.png)
<hr>

**ðŸŽ¯ Day 2 of #300DaysofData!:**
 
**Linear Regression Extensions:**<br> 
Today, I learned how to construct and implement the Extensions of Linear Regression which includes Ridge Regression, LASSO Regression, Bayesian Regression and Generalized Linear Models(GLMs) from the book **Machine Learning From Scratch**. I revised Matrix Calculus; dealing with multiple parameters, multiple observations, multiple loss functions and also Bayesian Estimators and Performance Estimation for Random Variables. Below is a snapshot of Regularised Regression, Bayesian Regression and Possion Regression(GLMs) using Python and Numpy Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Video:
  - [**Detection and Estimation Theory by Timothy Schulz**](https://www.youtube.com/playlist?list=PLXo-ki9WzLdjOFbvGz_TXCk7syaJ_2seq)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2b.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2c.png)
<hr>

**ðŸŽ¯ Day 3 of #300DaysofData!:**

**Discriminative Classifiers**<br>
Today, I learned how to construct and implement Binary and Multiple Logistic Regression, The Perceptron Algorithm, and Fisherâ€™s Linear Discriminant from the book **Machine Learning From Scratch**. I also read a blog post on the Math and Gradient Descent implementation of Multiclass Logistic Regression in Python by Sofia Yang. Below is a snapshot of Binary and Multiple Logistic Regression, The Perceptron Algorithm, and Fisherâ€™s Linear Discriminant using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Blog:
  - [**Multiclass Logistic Regression from Scratch**](https://towardsdatascience.com/multiclass-logistic-regression-from-scratch-9cc0007da372)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day3.png)
<hr>

**ðŸŽ¯ Day 4 of #300DaysofData!:**

**Generative Classifiers**<br>
Today, I read about Generative Classifiers which includes Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes and how to construct and implement them from the book **Machine Learning From Scratch**. I also read blog posts to further grasp the underlying Concepts and their Implications of Modelling Assumptions. Below is a snapshot of Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Blog:
  - [**Differences between LDA, QDA and Gaussian Naive Bayes classifiers**](https://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day4.png)
<hr>

**ðŸŽ¯ Day 5 of #300DaysofData!:**

**Decision Trees**:<br>
Today, I read about Decision Tree Regressor and Classifier, how it makes Predictions and its Interpretability, how it uses The CART Algorithm to split instances based on Low Mean Squared Error for Regression tasks, Gini Impurity and Entropy and also Regularization and Hyperparameter tuning, Pruning from the book **Machine Learning From Scratch** and **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Below is a snapshot of Decision Tree Regressor and Classifier implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day5.png)
<hr>

**ðŸŽ¯ Day 6 of #300DaysofData!:**

**Tree Ensemble Methods**:<br>
Ensemble Methods combine the outputs of multiple simple Models which is often called Learners in order to create the fine Model with low variance. Due to their high variance, a decision trees often fail to reach a level of precision comparable to other predictive algorithms and Ensemble Methods also known as Ensemble Learning Algorithms minimize the variance. Today I read and implemented Tree Ensemble Methods such as Bagging  Random Forests and Boosting experimenting with Base Estimators like Naive Bayes, Logistic Regression and Decision Tree, The Discrete AdaBoost Algorithm and AdaBoost for Regression along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. To further broaden my understanding, I used the book **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** to read on Voting Classifiers, Stacking, Out-of-Bag, Random Patches and Subspaces, Feature Importance, Regularization and Hyperparameter tuning of Ensemble Methods to which I applied to a project I have been working on, Exploring Nasa's Turbofan Dataset for Aircraft Predictive Maintenance. Below is a snapshot of Bagging, Random Forest and AdaBoost Classifier implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day6.png)

- Project Notebook: 
[**Turbofan Predictive Maintenance**](#)
<hr>


**ðŸŽ¯ Day 7 of #300DaysofData!:**

**Neural Networks**:<br>
A neural network in this case, artificial neurons called Artificial Neural Network (ANN) is an interconnected group of artificial neurons that uses a mathematical or computational model for information processing. In my journey of Machine Learning and Deep Learning, today I read and implemented Neural Networks along with the Construction of a Feed-Forward Neural Network with the Loop and Matrix Approach which constituted the Model Structure, Communication between Layers, Activation Functions, Optimization of Neural Nets using Back Propagation, Calculating Gradients and Combining Results with the Chain Rule from the same book **Machine Learning From Scratch** . Below is a snapshot of a Feed-Forward Neural Network implemented using Keras. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day7.png)
<hr>

**ðŸŽ¯ Day 8 of #300DaysofData!:**

**Gradient Descent and Cross Validation**:<br>
Gradient descent is an iterative approach to approximating the Parameters that Minimize a Differentiable Loss Function. Cross Validation is a Resampling Procedure used to evaluate Machine Learning Models on a limited Data sample which has a Parameter that splits the data into number of groups. Today, I read and implemented Gradient Descent and Cross validation along with the Construction from the book **Machine Learning From Scratch**. Below is a snapshot of Gradient Descent and Cross Validation using Python and the Numpy Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day8a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day8b.png)
<hr>

**ðŸŽ¯ Day 9 of #300DaysofData!:**

**Problem Framing and Fundermentals of Machine Learning**:<br>
Problem framing is the process of analyzing a problem to isolate the individual elements that need to be addressed to solve it and it also helps determine your project's technical feasibility and provides a clear set of goals and success criteria. Today, I read about Problem Framing, Understanding and Framing ML Problems and also the steps to take when tackling ML problems from Google's Machine learning Crash Course.Types of Machine Learning Systems: Supervised and Unsupervised Learning, Batch and Online Learning, Instance-Based Versus Model-Based Learning and also Main Challenges of ML from the book **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- [**Machine Learning Crash Course**](https://developers.google
<hr>

**ðŸŽ¯ Day 10 of #300DaysofData!:**

Today, I read about Classifications which entailed Training a Binary Classifier, Measuring Accuracy Using Cross-Validation, Multiclass, Multilabel and Multioutput Classification from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Since I have been learning from the same book for sometime now. I revised on Support Vector Machines, Desicion Trees, Ensemble Learning and Random Forests with their in-depth implementations. Below is repository containing the various notebooks including projects.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](#)
<hr>

**ðŸŽ¯ Day 11 of #300DaysofData!:**

Today, I read about The Curse of Dimensionality and how it has Effects on Machine Learning Models and the Techniques involved in tackling such problems. Projection and Manifold Learning Approach, PCA, Random Projection and LLE from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Since I have been learning from the same book for sometime now, I revised on Unsupervised Machine Learning: Clustering, Using KMeans, Anomaly Detection Using Gaussian Mixtures, Bayesian Gaussian Mixture Models, Anomaly and Novelty Detection with their in-depth implementations. Below is repository containing the related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)

<hr>

**ðŸŽ¯ Day 12 of #300DaysofData!:**

In my journey of Machine Learning and Deep Learning,
today, I read and implemented Artificial Neural Networks with Keras. Getting Started with The History Behind ANNs, Logical Computations with Neurons, Single Layered Perceptron, Multilayered Perceptron, Auto Diff, the COncept Behind Backpropagation, Regression and Classification MLPs. I also Built an Image Classifier Using the Sequential API, Complex Models Using the Functional API, Using the Subclassing API to Build Dynamic Models, Saving and Restoring a Model, Using Callbacks and TensorBoards. Fine-Tuning a Neural Network with Keras-Tuner from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Below is repository containing the related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)

<hr>

**ðŸŽ¯ Day 13 of #300DaysofData!:**

In my journey of Machine Learning and Deep Learning, today, I read and implemented Training Deep Neural Networks,
A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough
training instances or if th There Various Techniques in tackling these problems, i.e The Vanishing/Exploding Gradients
Problems which includes Glorot and He Initialization, Better Activation Functions such as Leaky RELU, Randomized Leaky ReLU (RReLU), SELU and ELUs, GELU, Swish and Mish, Batch Normalization and Gradient Clipping from the **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow**. I also revised on utilising Pretrained Models and Transfer Learning and how they make certain tasks easier, Faster Optimizers and How to avoid Overfitting when training. Below is repository containing the related notebooks.

- Book:
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

- Notebook:
  - [**handson-machinelearning**](https://github.com/DennisxB/handson-machinelearning.git)
