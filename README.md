# My Journey of 300DaysofData in Machine Learning and Deep Learning ðŸš€

**ðŸŽ¯ Day 1 of #300DaysofData!:**

**Ordinary Linear Regression**:<br>
In my journey of Machine Learning and Deep Learning, today I learned and implemented Ordinary Linear regression, Parameter Estimation, Minimizing Loss and Maximizing Likelihoods along with the construction and implementation of simple Linear Regression from the book **Machine Learning From Scratch**  and also an explanation of Performance Evaluation for the Estimation of Deterministic Parameters and Maximum Likelihood  Estimation for Multiple Regression from Timothy Schulz. I hope you will find some time reading the topics and books mentioned.
- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Video:
  - [**Detection and Estimation Theory by Timothy Schulz**](https://www.youtube.com/playlist?list=PLXo-ki9WzLdjOFbvGz_TXCk7syaJ_2seq)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day1a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day1b.png)
<hr>

**ðŸŽ¯ Day 2 of #300DaysofData!:**
 
**Linear Regression Extensions:**<br> 
Today, I learned how to construct and implement the Extensions of Linear Regression which includes Ridge Regression, LASSO Regression, Bayesian Regression and Generalized Linear Models(GLMs) from the book **Machine Learning From Scratch**. I revised Matrix Calculus; dealing with multiple parameters, multiple observations, multiple loss functions and also Bayesian Estimators and Performance Estimation for Random Variables. Below is a snapshot of Regularised Regression, Bayesian Regression and Possion Regression(GLMs) using Python and Numpy Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Video:
  - [**Detection and Estimation Theory by Timothy Schulz**](https://www.youtube.com/playlist?list=PLXo-ki9WzLdjOFbvGz_TXCk7syaJ_2seq)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2b.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day2c.png)
<hr>

**ðŸŽ¯ Day 3 of #300DaysofData!:**

**Discriminative Classifiers**<br>
Today, I learned how to construct and implement Binary and Multiple Logistic Regression, The Perceptron Algorithm, and Fisherâ€™s Linear Discriminant from the book **Machine Learning From Scratch**. I also read a blog post on the Math and Gradient Descent implementation of Multiclass Logistic Regression in Python by Sofia Yang. Below is a snapshot of Binary and Multiple Logistic Regression, The Perceptron Algorithm, and Fisherâ€™s Linear Discriminant using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Blog:
  - [**Multiclass Logistic Regression from Scratch**](https://towardsdatascience.com/multiclass-logistic-regression-from-scratch-9cc0007da372)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day3.png)
<hr>

**ðŸŽ¯ Day 4 of #300DaysofData!:**

**Generative Classifiers**<br>
Today, I read about Generative Classifiers which includes Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes and how to construct and implement them from the book **Machine Learning From Scratch**. I also read blog posts to further grasp the underlying Concepts and their Implications of Modelling Assumptions. Below is a snapshot of Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
- Blog:
  - [**Differences between LDA, QDA and Gaussian Naive Bayes classifiers**](https://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day4.png)
<hr>

**ðŸŽ¯ Day 5 of #300DaysofData!:**

**Decision Trees**:<br>
Today, I read about Decision Tree Regressor and Classifier, how it makes Predictions and its Interpretability, how it uses The CART Algorithm to split instances based on Low Mean Squared Error for Regression tasks, Gini Impurity and Entropy and also Regularization and Hyperparameter tuning, Pruning from the book **Machine Learning From Scratch** and **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**. Below is a snapshot of Decision Tree Regressor and Classifier implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day5.png)
<hr>

**ðŸŽ¯ Day 6 of #300DaysofData!:**

**Tree Ensemble Methods**:<br>
Ensemble Methods combine the outputs of multiple simple Models which is often called Learners in order to create the fine Model with low variance. Due to their high variance, a decision trees often fail to reach a level of precision comparable to other predictive algorithms and Ensemble Methods also known as Ensemble Learning Algorithms minimize the variance. Today I read and implemented Tree Ensemble Methods such as Bagging  Random Forests and Boosting experimenting with Base Estimators like Naive Bayes, Logistic Regression and Decision Tree, The Discrete AdaBoost Algorithm and AdaBoost for Regression along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. To further broaden my understanding, I used the book **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** to read on Voting Classifiers, Stacking, Out-of-Bag, Random Patches and Subspaces, Feature Importance, Regularization and Hyperparameter tuning of Ensemble Methods to which I applied to a project I have been working on, Exploring Nasa's Turbofan Dataset for Aircraft Predictive Maintenance. Below is a snapshot of Bagging, Random Forest and AdaBoost Classifier implemented using Scikit Learn Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day6.png)

- Project Notebook: 
[**Turbofan Predictive Maintenance**](#)
<hr>


**ðŸŽ¯ Day 7 of #300DaysofData!:**

**Neural Networks**:<br>
A neural network in this case, artificial neurons called Artificial Neural Network (ANN) is an interconnected group of artificial neurons that uses a mathematical or computational model for information processing. In my journey of Machine Learning and Deep Learning, today I read and implemented Neural Networks along with the Construction of a Feed-Forward Neural Network with the Loop and Matrix Approach which constituted the Model Structure, Communication between Layers, Activation Functions, Optimization of Neural Nets using Back Propagation, Calculating Gradients and Combining Results with the Chain Rule from the same book **Machine Learning From Scratch** . Below is a snapshot of a Feed-Forward Neural Network implemented using Keras. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day7.png)
<hr>

**ðŸŽ¯ Day 8 of #300DaysofData!:**

**Gradient Descent and Cross Validation**:<br>
Gradient descent is an iterative approach to approximating the Parameters that Minimize a Differentiable Loss Function. Cross Validation is a Resampling Procedure used to evaluate Machine Learning Models on a limited Data sample which has a Parameter that splits the data into number of groups. Today, I read and implemented Gradient Descent and Cross validation along with the Construction from the book **Machine Learning From Scratch** . Below is a snapshot of Gradient Descent and Cross Validation using Python and the Numpy Library. I hope you will find some to go through the resources mentioned.

- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day8a.png)
![Image](https://github.com/DennisxB/300Days__MachineLearningDeepLearning/blob/main/Images/Day8b.png)
